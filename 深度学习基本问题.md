# 深度学习基本问题



深度学习的基本问题是：学习一个由x到y的映射f

如果y是连续向量，则为回归问题

如果y是离散值，则为分类问题

如果y是随机变量，则为生成问题



为了学习这个映射f，首先要准备好一个映射库，

然后依据大量的数据，从映射库中搜索最接近目标映射f的假设h

这个搜索方法往往称为机器学习算法

![](pic\f.png)

在寻找目标函数这一基本问题下，又有三个子问题：

容量问题、收敛问题和优化问题



## 容量问题



上世纪八十年代提出的神经网络算法，是目前所有深度学习算法的基础

由于网上教程过多，所以不赘述整个算法流程





人工神经网络将目标函数的搜索过程转化为损失函数的优化问题



- **反向传播算法是梯度下降算法在多层网络结构中的实现算法**

  网络中的每个运算只需考虑自身的前向运算和反向运算，而不需要考虑和其他op之间的联系

  这不仅简化了计算而且简化了神经网络的实现

- 



#### 神经网络

这里主要指二层神经网络，也称作MLP，多层感知机网络

同样，教程很多，不赘述

其中有一些需要注意的：

- 反向传播
- 激活函数
- 欠拟合和过拟合

神经网络兴起于上个世纪80年代，它的出现掀起了人工智能的第二个浪潮，相比于逻辑回归，

神经网络明显提高了模型的容量

可以证明，在隐藏层神经元足够多的情况下，神经网络的容量是无穷的

这个结论被称为[普适逼近原理](https://en.wikipedia.org/wiki/Universal_approximation_theorem)



但是，单隐藏层，无限增加神经元的方案被证明是非常低效的，而且很容易造成严重的过拟合，

相比，多层网络的表现能力要优秀许多，加深模型成为了提高容量的唯一方案

30年前提出的神经网络，理论非常成熟，和现在的深度学习几乎没有区别，但人工智能技术在那时迎来了第二次漫长的寒冬，抛去客观因素不谈，在于加深模型的过程中出现了以下的问题：

- 计算力不够
- 数据不够
- 梯度弥散或梯度爆炸



#### 深度学习



06年，深度信念网络的提出，掀起了人工智能的第三波浪潮，

深度信念网络为了解决梯度传输的问题，提出了预训练，

但是人们发现，不需要预训练，从前那些不能跑的网络，其实都能跑，只是之前的计算能力不够

再后来relu激活函数的出现，基本解决了神经网络模型的梯度传输问题

模型从此越来越深，模型容量的问题基本解决



此时，真正的难题，就是过拟合，也就是说，如何在庞大的函数空间中找到想要的h呢

这里有几个讨论：

- 容量不是越大越好

  MLP的容量其实很大，但是在实际总是出现的非常严重的过拟合，所以目前的前沿结构中，

  MLP很少出现，或只是作为全连接层。从某一角度来说，容量越大，过拟合的风险越大，

  怎样找到一个合适的容量，没有通论。

- loss不是越低越好

  对于训练集来说，总是存在一个全局最优解，使loss最小，但是这个全局最优解，可能根本没有意义

  因为它代表着极端的过拟合。那么，让训练集的loss下降到什么程度最合适，训练多久最合适，没有通论

- 预训练或许有帮助

  目前非常大型的网络，都普遍结合了某种预训练技术，比如NLP领域的transformer，检测的RPN等。这其实非常直观，我们要让神经网络的求解过程，更接近人的认识过程

- 网络结构的调整

  和上面的思路相近，卷积神经网络很接近人的认识图像的过程，效果非常好。所以卷积神经网络的兴起，不是因为这个网络的容量大，而是因为能够很好的解决计算机视觉领域特定任务的过拟合问题

- 数据量一定会有帮助

  数据量越大，越能体现目标函数的特征



####深度学习和通信结合

深度学习，可以理解是加强版的神经网络

但是却带了前所未有的变化：

- 理念上的变化，即数据驱动，端到端
- 基础设施的变革，包括高速通信，云计算技术，芯片等
- 推动了人工智能的发展，深度学习其实只是人工智能领域里一个很小的分支
  - 符号派，模拟人的认知过程，知识图谱
  - 连接派，模拟人脑的思维过程，深度学习
  - 控制派，模拟人的行为，强化学习



在这种大环境下，深度学习怎样和通信的物理层结合

1. 改良现有通信技术：盲算法都改为不盲算法，自适应算法都改为智能自适应算法等
2. 模拟平台：核心是信道的建模，结合当前的仿真技术，端到端
3. 硬件输出：能够物理实现光计算AI芯片





